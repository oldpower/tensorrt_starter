"""
è¯´æ˜ï¼š

# 1. ç¡®ä¿ä½ æœ‰ pip å’Œ Pythonï¼ˆ3.8~3.11ï¼‰ æµ‹è¯•ä½¿ç”¨äº†3.11
python --version

# 2. å®‰è£… tensorrt-cu12ï¼ˆè‡ªåŠ¨åŒ…å«æ‰€æœ‰ä¾èµ–ï¼‰
pip install tensorrt-cu12
# æˆ–ä» tar æ–‡ä»¶å®‰è£…ï¼ˆä¸‹è½½è‡ª NVIDIA Developerï¼‰,æŒ‡å®šç‰ˆæœ¬
# pip install tensorrt-8.x.x.x-cp3x-none-linux_x86_64.whl

# 3. ï¼ˆå¯é€‰ï¼‰å®‰è£… pycudaï¼Œç”¨äº GPU å†…å­˜ç®¡ç†
pip install pycuda

# 4. å…¶ä»–
pip install Pillow
pip install torch torchvision

Installing collected packages: nvidia-cusparselt-cu12, mpmath, triton, sympy, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufil
e-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, jinja2, fsspec, filelock, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudn
n-cu12, nvidia-cusolver-cu12, torch, torchvision
  Attempting uninstall: nvidia-cuda-runtime-cu12
    Found existing installation: nvidia-cuda-runtime-cu12 12.9.79
    Uninstalling nvidia-cuda-runtime-cu12-12.9.79:
      Successfully uninstalled nvidia-cuda-runtime-cu12-12.9.79
Successfully installed filelock-3.20.0 fsspec-2025.9.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.5 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia
-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu1
2-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.3.20 nvidia-nvtx-cu12-12.8.90 sympy-1.14.0 torch-2.9.0 torchvision-0.24.0
 triton-3.5.0

pip install opencv-python
pip install onnxruntime
pip install onnx

---
# å¯ä¼˜åŒ–é¡¹
 - è¾“å…¥æ•°æ®å½’ä¸€åŒ–ä¸ä½¿ç”¨torchvision
 - softmax ä¸ä½¿ç”¨ torch
 - infer è¿ç»­æ¨ç†ï¼Œç›¸åŒè¾“å…¥å°ºå¯¸ä¸‹ï¼Œå†…å­˜é¢„åˆ†é…å’Œå¤ç”¨

"""

import tensorrt as trt
import pycuda.driver as cuda
import pycuda.autoinit
import numpy as np

import torch
from torchvision import transforms
from PIL import Image
import time
import os
current_dir = os.path.dirname(os.path.abspath(__file__))

def onnx2engine():

    # 1 åˆå§‹åŒ–èµ„æº
    # åˆå§‹åŒ– Logger å’Œ Builder
    logger = trt.Logger(trt.Logger.WARNING)
    builder = trt.Builder(logger)

    # åˆ›å»ºç½‘ç»œå®šä¹‰ï¼ˆå¯ç”¨æ˜¾å¼æ‰¹å¤„ç†ï¼‰
    network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
    config = builder.create_builder_config()

    # è®¾ç½® workspace å¤§å°ï¼ˆä¾‹å¦‚ 2GBï¼‰
    config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 2 * 1024 * 1024 * 1024)

    # å¯ç”¨ FP16ï¼ˆå¦‚æœç¡¬ä»¶æ”¯æŒï¼‰
    if builder.platform_has_fast_fp16:
        config.set_flag(trt.BuilderFlag.FP16)

    # åŠ è½½ ONNX æ¨¡å‹
    parser = trt.OnnxParser(network, logger)
    onnx_file_path = "./models/vit_b_32_2class.onnx"

    with open(onnx_file_path, "rb") as f:
        if not parser.parse(f.read()):
            print("âŒ Failed to parse ONNX file:")
            for i in range(parser.num_errors):
                print(parser.get_error(i))
            exit(1)

    print("âœ… ONNX parsed successfully.")

    # 2 é…ç½®profile
    # è·å–è¾“å…¥å¼ é‡
    input_tensor = network.get_input(0)  # æˆ–é€šè¿‡åå­—: network.get_input_by_name("input")
    input_name = input_tensor.name
    print(f"Input name: {input_name}, shape: {input_tensor.shape}")

    # åˆ›å»ºä¼˜åŒ– profile
    profile = builder.create_optimization_profile()

    # è®¾ç½®åŠ¨æ€ç»´åº¦èŒƒå›´
    # set_shape(name, min, opt, max)
    #   - min: æœ€å° shapeï¼ˆä¿è¯æ”¯æŒï¼‰
    #   - opt: æœ€ä¼˜ shapeï¼ˆTensorRT ä¸ºæ­¤ä¼˜åŒ– kernelï¼‰
    #   - max: æœ€å¤§ shapeï¼ˆä¸èƒ½è¶…è¿‡ï¼‰
    # profile.set_shape(
    #     input_name, 
    #     min=(1, 3, 256, 256),    # æœ€å°è¾“å…¥
    #     opt=(4, 3, 512, 512),    # å¸¸è§è¾“å…¥ï¼ˆæ€§èƒ½ä¸ºæ­¤ä¼˜åŒ–ï¼‰
    #     max=(8, 3, 1024, 1024)   # æœ€å¤§è¾“å…¥
    # )
    profile.set_shape(
        input_name,
        min=(1, 3, 224, 224),   # batch=1, å…¶ä»–å›ºå®š
        opt=(4, 3, 224, 224),   # batch=4ï¼ˆæœ€å¸¸è§ï¼‰
        max=(8, 3, 224, 224)    # batch=8
    )

    # å°† profile æ·»åŠ åˆ° config
    config.add_optimization_profile(profile)

    print("âœ… Optimization profile added for dynamic shape.")



    # 3 æ„å»ºengine
    print("ğŸ”§ Building serialized engine...")
    engine_bytes = builder.build_serialized_network(network, config)

    if engine_bytes is None:
        print("âŒ Engine build failed.")
        exit(1)

    # ä¿å­˜ engine æ–‡ä»¶
    engine_file_path = "./build/engines/pythontrt.engine"
    with open(engine_file_path, "wb") as f:
        f.write(engine_bytes)
    print(f"âœ… Serialized engine saved to {engine_file_path}")

def inference_trt():
    runtime = trt.Runtime(trt.Logger(trt.Logger.WARNING))
    with open("./build/engines/pythontrt.engine", "rb") as f:
        engine = runtime.deserialize_cuda_engine(f.read())

    # âœ… ç”¨ with ç¡®ä¿ context è¢«æ­£ç¡®é”€æ¯
    with engine.create_execution_context() as context:
        # âœ… è®¾ç½®ä¼˜åŒ– profileï¼ˆå¿…é¡»åœ¨è®¾ç½® shape å‰ï¼‰
        context.set_optimization_profile_async(0, 0)

        input_shape = (3, 3, 224, 224)
        # context.set_binding_shape(0, input_shape)
        context.set_input_shape("input0", input_shape)
        actual_shape = context.get_tensor_shape("input0")
        print(f"input0 shape: {actual_shape}") 

        host_input = np.random.rand(*input_shape).astype(np.float32)
        device_input = cuda.mem_alloc(host_input.nbytes)

        output_shape = tuple(context.get_tensor_shape("output0"))
        print(f"output0 shape : {output_shape}")
        host_output = cuda.pagelocked_empty(output_shape, dtype=np.float32)
        device_output = cuda.mem_alloc(host_output.nbytes)

        # âœ… è®¾ç½® tensor åœ°å€ï¼ˆå…³é”®ï¼ï¼‰
        context.set_tensor_address("input0", device_input)
        context.set_tensor_address("output0", device_output)

        stream = cuda.Stream()
        cuda.memcpy_htod_async(device_input, host_input, stream)
        context.execute_async_v3(stream.handle)
        cuda.memcpy_dtoh_async(host_output, device_output, stream)
        stream.synchronize()
        
        print("âœ… Inference with dynamic shape completed.")
        print(host_output)

        # âœ… æ˜¾å¼é‡Šæ”¾ GPU å†…å­˜
        device_input.free()
        device_output.free()


class TRTInfer:
    def __init__(self, engine_path):
        self.logger = trt.Logger(trt.Logger.WARNING)
        self.runtime = trt.Runtime(self.logger)

        with open(engine_path, "rb") as f:
            self.engine = self.runtime.deserialize_cuda_engine(f.read())

        self.context = self.engine.create_execution_context()
        self.stream = cuda.Stream()

        # ğŸ” è‡ªåŠ¨æå–æ‰€æœ‰è¾“å…¥å’Œè¾“å‡ºçš„åå­—
        self.input_names = []
        self.output_names = []

        for i in range(self.engine.num_io_tensors):
            name = self.engine.get_tensor_name(i)
            if self.engine.get_tensor_mode(name) == trt.TensorIOMode.INPUT:
                self.input_names.append(name)
            elif self.engine.get_tensor_mode(name) == trt.TensorIOMode.OUTPUT:
                self.output_names.append(name)

        print(f"âœ… Engine loaded with {len(self.input_names)} inputs: {self.input_names}")
        print(f"âœ… And {len(self.output_names)} outputs: {self.output_names}")

        # ç”¨äºç®¡ç†å†…å­˜
        self.device_inputs = {}
        self.device_outputs = {}
        self.host_outputs = {}

        # ğŸ‘‡ è®°å½•æ˜¯å¦å·²åˆ†é…å†…å­˜,è®°å½•è¾“å…¥/è¾“å‡ºåˆ†é…çš„å­—èŠ‚æ•°ï¼ˆç”¨äºåˆ¤æ–­æ˜¯å¦éœ€è¦ reallocï¼‰
        self._allocated = False
        self.input_sizes = {}
        self.output_sizes = {}

    def infer(self, input_data):
        """
        æ¨ç†å‡½æ•°
        :param input_data: å¯ä»¥æ˜¯ï¼š
            - dict: {input_name: np.ndarray}
            - np.ndarray: è‡ªåŠ¨ç»‘å®šåˆ°ç¬¬ä¸€ä¸ªè¾“å…¥
        """
        # === å¤„ç†è¾“å…¥ï¼šç»Ÿä¸€ä¸º dict æ ¼å¼ ===
        if isinstance(input_data, dict):
            input_data_dict = input_data
        elif isinstance(input_data, np.ndarray):
            if len(self.input_names) != 1:
                raise ValueError(f"Model has {len(self.input_names)} inputs, but only one array provided. Use dict format.")
            input_data_dict = {self.input_names[0]: input_data}
        else:
            raise TypeError("input_data must be a dict or a numpy array")

        # === æ£€æŸ¥æ‰€æœ‰è¾“å…¥æ˜¯å¦éƒ½æä¾›äº† ===
        for name in self.input_names:
            if name not in input_data_dict:
                raise KeyError(f"Missing input tensor: '{name}'. Provided keys: {list(input_data_dict.keys())}")

        # è®¾ç½® profile
        self.context.set_optimization_profile_async(0, 0)

        # === ç¬¬ä¸€æ­¥ï¼šè®¾ç½®æ¯ä¸ªè¾“å…¥çš„ shapeï¼Œå¹¶åˆ†é…æˆ–å¤ç”¨æ˜¾å­˜ ===
        # åˆå§‹åŒ– input_sizes å­—å…¸ï¼ˆç¬¬ä¸€æ¬¡è¿è¡Œæ—¶ï¼‰
        # if not hasattr(self, 'input_sizes'):
        #     self.input_sizes = {}
        for name in self.input_names:
            data = input_data_dict[name]  # è·å–å¯¹åº”çš„ numpy è¾“å…¥
            shape = data.shape

            # è®¾ç½®è¾“å…¥ shapeï¼ˆåŠ¨æ€ shape æ”¯æŒï¼‰
            self.context.set_input_shape(name, shape)

            # æ£€æŸ¥æ˜¯å¦æˆåŠŸè®¾ç½®
            actual_shape = tuple(self.context.get_tensor_shape(name))
            if -1 in actual_shape:
                raise ValueError(f"Shape for input '{name}' not fully specified: {actual_shape}")

            # åˆ†é… GPU å†…å­˜ï¼ˆå¦‚æœä¹‹å‰æ²¡åˆ†é…è¿‡ï¼Œæˆ– shape å˜äº†ï¼‰
            nbytes = data.nbytes
            if name not in self.device_inputs or self.input_sizes.get(name, 0) < nbytes:
                if name in self.device_inputs:
                    self.device_inputs[name].free()
                self.device_inputs[name] = cuda.mem_alloc(nbytes)
                self.input_sizes[name] = nbytes     # âœ… è®°å½•å¤§å°

            # âœ… ç»‘å®š tensor åœ°å€
            self.context.set_tensor_address(name, self.device_inputs[name])

        # === ç¬¬äºŒæ­¥ï¼šä¸ºæ¯ä¸ªè¾“å‡ºåˆ†é…å†…å­˜å¹¶ç»‘å®šåœ°å€ ===
        if not self._allocated:
            for name in self.output_names:
                shape = tuple(self.context.get_tensor_shape(name))
                dtype = np.float32  # å‡è®¾è¾“å‡ºæ˜¯ float32ï¼Œå¯æ ¹æ®éœ€è¦è°ƒæ•´
                host_buf = cuda.pagelocked_empty(shape, dtype=dtype)
                device_buf = cuda.mem_alloc(host_buf.nbytes)

                self.host_outputs[name] = host_buf
                self.device_outputs[name] = device_buf
                self.output_sizes[name] = host_buf.nbytes  # âœ… è®°å½•å¤§å°

                # âœ… ç»‘å®šè¾“å‡º tensor åœ°å€
                self.context.set_tensor_address(name, device_buf)
            self._allocated = True
        else:
            # âœ… åç»­æ¨ç†ï¼šæ£€æŸ¥ shape æ˜¯å¦å˜åŒ–ï¼Œè‹¥å˜å¤§åˆ™é‡æ–°åˆ†é…
            for name in self.output_names:
                shape = tuple(self.context.get_tensor_shape(name))
                dtype = np.float32
                nbytes = int(np.prod(shape)) * np.dtype(dtype).itemsize

                if self.output_sizes[name] < nbytes:
                    # é‡Šæ”¾æ—§å†…å­˜
                    self.device_outputs[name].free()
                    # é‡æ–°åˆ†é…
                    self.host_outputs[name] = cuda.pagelocked_empty(shape, dtype=dtype)
                    self.device_outputs[name] = cuda.mem_alloc(nbytes)
                    self.output_sizes[name] = nbytes

                # âœ… æ¯æ¬¡å¿…é¡»é‡æ–°ç»‘å®šåœ°å€ï¼ˆå› ä¸º context å¯èƒ½å˜äº†ï¼‰
                self.context.set_tensor_address(name, self.device_outputs[name])

        # === ç¬¬ä¸‰æ­¥ï¼šæ•°æ®æ‹·è´ï¼ˆHost -> Deviceï¼‰===
        for name in self.input_names:
            cuda.memcpy_htod_async(self.device_inputs[name], input_data_dict[name], self.stream)

        # === ç¬¬å››æ­¥ï¼šæ‰§è¡Œæ¨ç† ===
        self.context.execute_async_v3(self.stream.handle)

        # === ç¬¬äº”æ­¥ï¼šæ‹·è´è¾“å‡ºç»“æœï¼ˆDevice -> Hostï¼‰===
        for name in self.output_names:
            cuda.memcpy_dtoh_async(self.host_outputs[name], self.device_outputs[name], self.stream)

        # åŒæ­¥æµ
        self.stream.synchronize()

        # è¿”å›è¾“å‡ºå­—å…¸
        return {name: self.host_outputs[name].copy() for name in self.output_names}  # .copy() ç¡®ä¿è„±ç¦» pagelocked memory

    def cleanup(self):
        """æ‰‹åŠ¨æ¸…ç† GPU å†…å­˜"""
        for buf in self.device_inputs.values():
            buf.free()
        for buf in self.device_outputs.values():
            buf.free()
        self.device_inputs.clear()
        self.device_outputs.clear()

    def __del__(self):
        self.cleanup()
        try:
            del self.context
            del self.engine
            del self.runtime
        except Exception as _:
            pass

infer = TRTInfer('./build/engines/pythontrt.engine')
def inference_TRTInfer():
    pil_image = Image.open(os.path.join(current_dir,'./assets/StirringSL/20250814-StirringSolidLiquid_frame_0000.png'))
    transform = transforms.Compose([
        transforms.Resize((224,224)),  # ViT-B/32 ä½¿ç”¨ 224x224
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])
    input_tensor = transform(pil_image).unsqueeze(0)  # æ·»åŠ  batch ç»´åº¦
    # input_tensor = torch.cat([input_tensor, input_tensor], dim=0)
    inputs = input_tensor.numpy()

    starttime = time.time()
    for _ in range(5):
        # æ¨ç†
        outputs = infer.infer(inputs)['output0']
        print(f"â°trtengineæ¨ç†è€—æ—¶: {time.time() - starttime:.4f} s")

        # è·å–è¾“å‡º
        logits = outputs  # shape: [1, num_classes]
        # print("åŸå§‹è¾“å‡º:", logits)
        # è½¬ä¸ºæ¦‚ç‡ï¼ˆSoftmaxï¼‰
        probabilities = torch.softmax(torch.from_numpy(logits), dim=1).numpy()[0]
        # print("æ¦‚ç‡åˆ†å¸ƒ:", probabilities)
        # è·å–é¢„æµ‹ç±»åˆ«
        predicted_class = probabilities.argmax()
        # print(f"é¢„æµ‹ç±»åˆ«: {predicted_class}, ç½®ä¿¡åº¦: {probabilities[predicted_class]:.4f}")

if __name__ == "__main__":
    # onnx2engine()
    # inference_trt()
    inference_TRTInfer()
    
